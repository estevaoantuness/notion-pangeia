"""
Agente Conversacional com GPT-4o-mini - Conversational Agent.

Sistema de agente que usa LLM (GPT-4o-mini) para gerar respostas naturais
mantendo contexto psicol√≥gico e intelig√™ncia emocional.
"""

import logging
import json
from typing import Optional, Tuple, Dict, List
from datetime import datetime, timedelta

from config.openai_config import (
    client, GPT_MODEL, OPENAI_API_KEY, MAX_CONTEXT_TOKENS, MAX_RESPONSE_TOKENS,
    TEMPERATURE, TOP_P, REQUEST_TIMEOUT, MAX_CONVERSATION_HISTORY,
    SYSTEM_PROMPT_TEMPLATE, FALLBACK_RESPONSES, LOG_CONVERSATIONS,
    APIError, APIConnectionError, RateLimitError, AuthenticationError
)
from src.psychology.engine import PsychologicalEngine, EmotionalState
from src.people.analytics import PeopleAnalytics
from src.notion.tasks import TasksManager
from src.interventions.nudge_engine import NudgeEngine
from src.interventions.personalization import PersonalizationEngine
from src.memory.redis_manager import get_memory_manager
from src.memory.conversation_state import get_conversation_state
from src.agents.task_manager_agent import TaskManagerAgent

# Intelligence Module - Terapeuta Produtivo
from src.intelligence import (
    ToneDetector, PatternDetector, ProactiveEngine,
    EmotionalCorrelation, get_conversation_memory
)

logger = logging.getLogger(__name__)


class ConversationalAgent:
    """
    Agente conversacional com mem√≥ria e contexto psicol√≥gico.

    Usa GPT-4o-mini para gerar respostas naturais enquanto mant√©m:
    - Hist√≥rico de conversa
    - Contexto psicol√≥gico
    - An√°lise emocional
    - Limita√ß√µes de tokens
    """

    def __init__(self):
        """Inicializa o agente conversacional."""
        self.psych_engine = PsychologicalEngine()
        self.analytics = PeopleAnalytics()
        self.tasks_manager = TasksManager()
        self.nudge_engine = NudgeEngine()
        self.personalization_engine = PersonalizationEngine()
        self.task_manager_agent = TaskManagerAgent()

        # Memory compartilhada entre workers via Redis
        self.memory_manager = get_memory_manager()

        # Conversation state para tracking de contexto
        self.conversation_state = get_conversation_state()

        # Intelligence Module - Terapeuta Produtivo
        self.tone_detector = ToneDetector()
        self.pattern_detector = PatternDetector()
        self.proactive_engine = ProactiveEngine()
        self.emotional_correlation = EmotionalCorrelation()
        self.long_term_memory = get_conversation_memory(
            storage_backend=self.memory_manager.redis if self.memory_manager.is_redis_available() else None
        )

        # Tracking de custos
        self.cost_tracking: Dict[str, float] = {}

        # √öltimas respostas (para evitar repeti√ß√£o)
        self.response_cache: Dict[str, Tuple[str, datetime]] = {}

        storage_type = "Redis" if self.memory_manager.is_redis_available() else "Local (warning: n√£o persiste entre workers)"
        logger.info(f"ConversationalAgent inicializado - Terapeuta Produtivo com Intelligence Module ({storage_type})")

    def get_conversation_history(self, user_id: str) -> List[Dict]:
        """Obt√©m hist√≥rico da conversa do usu√°rio."""
        return self.memory_manager.get_history(user_id, limit=MAX_CONVERSATION_HISTORY)

    def add_to_memory(self, user_id: str, role: str, content: str) -> None:
        """Adiciona mensagem ao hist√≥rico."""
        self.memory_manager.add_message(
            user_id=user_id,
            role=role,
            content=content,
            max_messages=MAX_CONVERSATION_HISTORY
        )

    def _safe_analyze_user(self, person_name: str) -> Dict:
        """
        Wrapper seguro para an√°lise psicol√≥gica do usu√°rio.
        Retorna dicion√°rio com dados mesmo se an√°lise falhar.
        """
        try:
            # Busca tarefas reais do Notion
            tasks = self.tasks_manager.get_person_tasks(person_name, include_completed=True)
            progress = self.tasks_manager.calculate_progress(person_name)

            # Conta tarefas por status
            total_pendentes = len(tasks.get("a_fazer", [])) + len(tasks.get("em_andamento", []))
            em_andamento = len(tasks.get("em_andamento", []))
            a_fazer = len(tasks.get("a_fazer", []))
            concluidas = progress.get("concluidas", 0)
            total = progress.get("total", 0)
            percentual = progress.get("percentual", 0)

            # Monta descri√ß√£o das tarefas ativas
            if total_pendentes == 0:
                active_tasks = "nenhuma tarefa pendente no momento"
            elif total_pendentes == 1:
                active_tasks = "1 tarefa pendente"
            else:
                active_tasks = f"{total_pendentes} tarefas pendentes ({em_andamento} em andamento, {a_fazer} a fazer)"

            # Monta descri√ß√£o do progresso
            if total == 0:
                progress_desc = "sem tarefas registradas no momento"
            elif percentual == 100:
                progress_desc = "todas as tarefas conclu√≠das! üéâ"
            elif percentual >= 80:
                progress_desc = f"excelente progresso: {concluidas} de {total} tarefas conclu√≠das ({percentual}%)"
            elif percentual >= 50:
                progress_desc = f"bom progresso: {concluidas} de {total} tarefas conclu√≠das ({percentual}%)"
            elif percentual >= 25:
                progress_desc = f"progresso moderado: {concluidas} de {total} tarefas conclu√≠das ({percentual}%)"
            else:
                progress_desc = f"in√≠cio do dia: {concluidas} de {total} tarefas conclu√≠das ({percentual}%)"

            # Determina n√≠vel de energia baseado na carga de trabalho
            if total_pendentes == 0:
                energy_level = "tranquila"
            elif total_pendentes <= 3:
                energy_level = "boa"
            elif total_pendentes <= 7:
                energy_level = "moderada"
            else:
                energy_level = "alta carga"

            # Estado emocional baseado no progresso
            if percentual >= 80:
                emotional_state = "Motivado"
            elif percentual >= 50:
                emotional_state = "Equilibrado"
            elif total_pendentes > 10:
                emotional_state = "Sobrecarregado"
            else:
                emotional_state = "Tranquilo"

            return {
                "emotional_state": emotional_state,
                "energy_level": energy_level,
                "active_tasks": active_tasks,
                "progress": progress_desc
            }

        except Exception as e:
            logger.error(f"Erro ao analisar usu√°rio {person_name}: {e}", exc_info=True)
            # Retorna dados padr√£o em caso de erro
            return {
                "emotional_state": "Tranquilo",
                "energy_level": "boa",
                "active_tasks": "algumas tarefas",
                "progress": "em progresso"
            }

    def build_system_prompt(self, person_name: str, user_id: str, history: List[Dict], message: str = "") -> str:
        """Constr√≥i system prompt personalizado com contexto inteligente."""
        try:
            # An√°lise psicol√≥gica segura (sempre retorna dados)
            metrics = self._safe_analyze_user(person_name)

            # Detecta tom emocional da √∫ltima mensagem (se houver)
            detected_tone = "Neutro"
            if message:
                tone_analysis = self.tone_detector.detect_tone(message)
                detected_tone = f"{tone_analysis.primary_tone.value.title()} ({int(tone_analysis.confidence*100)}% confian√ßa)"

                # Aprende prefer√™ncias de comunica√ß√£o se alta confian√ßa
                if tone_analysis.confidence >= 0.7:
                    self.long_term_memory.learn_preference(
                        user_id=user_id,
                        key="recent_tone",
                        value=tone_analysis.primary_tone.value,
                        confidence=tone_analysis.confidence
                    )

            # Pega resumo de mem√≥ria de longo prazo
            memory_summary = self.long_term_memory.get_conversation_summary(user_id)

            # Formata contexto de conversa
            conversation_context = ""
            if memory_summary.get("top_topics"):
                topics = ", ".join([t["topic"] for t in memory_summary["top_topics"][:3]])
                conversation_context = f"**T√≥picos j√° discutidos:** {topics}\n"

            # Detecta padr√µes comportamentais (se houver hist√≥rico suficiente)
            detected_patterns = ""
            try:
                # TODO: Implementar get_metrics_history e get_tasks_history
                # Por enquanto, retorna vazio para n√£o quebrar
                metrics_history = []  # self.analytics.get_metrics_history(person_name, days=30)
                tasks_history = []  # self.tasks_manager.get_tasks_history(person_name, days=30)

                if len(metrics_history) >= 7:  # M√≠nimo de dados
                    patterns = self.pattern_detector.detect_all_patterns(
                        person_name=person_name,
                        tasks_history=tasks_history,
                        metrics_history=metrics_history
                    )

                    if patterns:
                        # Pega o padr√£o de maior confian√ßa
                        top_pattern = max(patterns, key=lambda p: p.confidence)
                        detected_patterns = (
                            f"**‚ö†Ô∏è Padr√£o Detectado ({int(top_pattern.confidence*100)}%):**\n"
                            f"{top_pattern.pattern_type.value.replace('_', ' ').title()}\n"
                            f"Use esse insight proativamente se relevante para a conversa.\n"
                        )
            except Exception as e:
                logger.debug(f"N√£o foi poss√≠vel detectar padr√µes: {e}")

            # Preenche template com dados seguros
            prompt = SYSTEM_PROMPT_TEMPLATE.format(
                name=person_name or "Amigo",
                emotional_state=metrics.get("emotional_state", "Tranquilo"),
                energy_level=metrics.get("energy_level", "boa"),
                active_tasks=metrics.get("active_tasks", "algumas tarefas"),
                progress=metrics.get("progress", "em progresso"),
                detected_tone=detected_tone,
                conversation_context=conversation_context,
                detected_patterns=detected_patterns
            )

            # Adicionar contexto de conversa
            context_summary = self.conversation_state.build_context_summary(user_id)
            if context_summary:
                prompt += f"\n\n{context_summary}"

            # Extrair contexto do hist√≥rico e atualizar estado
            context = self.conversation_state.extract_context_from_history(user_id, history)

            # Adicionar nota sobre evitar repeti√ß√µes
            if context.get("questions_asked_count", 0) > 1:
                prompt += "\n\n**IMPORTANTE**: J√° houve conversa anterior. Evite repetir perguntas. Use o contexto!"

            # Adicionar refer√™ncia ao √∫ltimo sentimento mencionado
            if context.get("user_mentioned_feeling"):
                feeling = context["user_mentioned_feeling"]
                prompt += f"\n\n**CONTEXTO**: Usu√°rio mencionou que est√° {feeling}. Fa√ßa refer√™ncia a isso!"

            return prompt

        except Exception as e:
            logger.error(f"Erro ao construir system prompt: {e}")
            # Fallback seguro
            return SYSTEM_PROMPT_TEMPLATE.format(
                name=person_name or "Amigo",
                emotional_state="Tranquilo",
                energy_level="boa",
                active_tasks="algumas tarefas",
                progress="em progresso",
                detected_tone="Neutro",
                conversation_context="",
                detected_patterns=""
            )

    def generate_response(self, user_id: str, message: str, person_name: str) -> Tuple[bool, str]:
        """
        Gera resposta usando GPT-4o-mini com contexto conversacional.

        Args:
            user_id: ID √∫nico do usu√°rio
            message: Mensagem recebida
            person_name: Nome da pessoa

        Returns:
            Tuple (sucesso, resposta)
        """
        try:
            # PRIMEIRO: Verificar se √© comando de task (prioridade!)
            task_result = self.task_manager_agent.process_message(person_name, message)

            if task_result:
                success, response_text = task_result

                # Adiciona mensagens ao hist√≥rico
                self.add_to_memory(user_id, "user", message)
                self.add_to_memory(user_id, "assistant", response_text)

                logger.info(f"Comando de task processado para {person_name}")
                return success, response_text

            # Verificar se cliente est√° dispon√≠vel
            if not client:
                logger.warning("‚ö†Ô∏è Cliente OpenAI n√£o inicializado - usando fallback")
                return self._generate_fallback_response(message, person_name)

            # Adiciona mensagem do usu√°rio ao hist√≥rico
            self.add_to_memory(user_id, "user", message)

            # Obt√©m hist√≥rico
            history = self.get_conversation_history(user_id)

            # Constr√≥i system prompt personalizado com contexto (passando mensagem para an√°lise de tom)
            system_prompt = self.build_system_prompt(person_name, user_id, history, message)

            # Verifica cache para evitar resposta duplicada
            cache_key = f"{user_id}:{message}"
            if cache_key in self.response_cache:
                cached_response, timestamp = self.response_cache[cache_key]
                if (datetime.now() - timestamp).seconds < 60:  # Cache de 1 minuto
                    logger.info(f"Resposta do cache para {person_name}")
                    return True, cached_response

            # Chamada ao GPT-4o-mini com novo cliente (v1.0+)
            logger.info(f"Gerando resposta com GPT-4o-mini para {person_name}")

            response = client.chat.completions.create(
                model=GPT_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    *[{"role": msg["role"], "content": msg["content"]} for msg in history],
                ],
                temperature=TEMPERATURE,
                top_p=TOP_P,
                max_tokens=MAX_RESPONSE_TOKENS,
                timeout=REQUEST_TIMEOUT,
            )

            # Extrai resposta
            bot_response = response.choices[0].message.content.strip()

            # Adiciona resposta do bot ao hist√≥rico
            self.add_to_memory(user_id, "assistant", bot_response)

            # Calcula e registra tokens usados
            if hasattr(response, 'usage'):
                tokens_used = response.usage.total_tokens
                logger.info(f"Tokens utilizados: {tokens_used}")
                self._track_cost(user_id, tokens_used)

            # Cache a resposta
            self.response_cache[cache_key] = (bot_response, datetime.now())

            # Log da conversa se habilitado
            if LOG_CONVERSATIONS:
                logger.info(f"[{person_name}] User: {message} | Bot: {bot_response[:100]}...")

            return True, bot_response

        except AuthenticationError:
            logger.error(f"Erro de autentica√ß√£o OpenAI - verifique OPENAI_API_KEY")
            return self._generate_fallback_response(message, person_name)

        except RateLimitError:
            logger.warning(f"Rate limit atingido para {person_name}")
            return False, FALLBACK_RESPONSES["overload"]

        except APIConnectionError as e:
            logger.warning(f"Erro de conex√£o com OpenAI para {person_name}: {e}")
            return False, FALLBACK_RESPONSES["timeout"]

        except APIError as e:
            logger.error(f"Erro na API OpenAI: {e}")
            return self._generate_fallback_response(message, person_name)

        except Exception as e:
            logger.error(f"Erro ao gerar resposta: {e}")
            return self._generate_fallback_response(message, person_name)

    def _generate_fallback_response(self, message: str, person_name: str) -> Tuple[bool, str]:
        """
        Gera resposta de fallback quando GPT-4o-mini n√£o est√° dispon√≠vel.
        Foca no M√©todo Pangeia e capacidades do sistema.
        """
        try:
            message_lower = message.lower().strip()

            # Respostas de sauda√ß√£o - Filosofia Pangeia mas com empatia
            if any(word in message_lower for word in ["oi", "opa", "ol√°", "e a√≠", "eae"]):
                responses = [
                    f"E a√≠, {person_name}! üåç\n\nVi suas tasks aqui. Antes de falar delas, queria saber: como voc√™ t√°?\n\nPangeia = CUIDAR primeiro, tasks depois.",
                    f"Opa, {person_name}!\n\nPange.IA aqui - ajudo voc√™ a focar no que REALMENTE importa.\n\nComo t√° sua energia hoje? [1-10]\n\n(Porque se t√° baixa, vamos ajustar sua lista antes)",
                    f"Oi, {person_name}! üåç\n\nAntes de qualquer coisa: voc√™ t√° bem?\n\nSe n√£o tiver 100%, a gente ajusta suas tasks. CUIDAR vem primeiro sempre.",
                ]
                return True, responses[hash(message) % len(responses)]

            elif any(word in message_lower for word in ["tchau", "at√©", "falou", "bye"]):
                responses = [
                    f"Falou, {person_name}! üåç\n\nSe for descansar, descansa DE VERDADE. Nada de ficar pensando em trampo.\n\nAt√©!",
                    f"At√©! Lembra: fazer NADA de vez em quando √© produtivo tamb√©m. Pangeia aprova pausas reais! üåç",
                ]
                return True, responses[hash(message) % len(responses)]

            elif any(word in message_lower for word in ["obrigado", "obg", "vlw", "valeu"]):
                return True, f"Disponha. Pangeia √© sobre simplificar, n√£o complicar. Se eu te ajudei a ELIMINAR algo, valeu muito. üåç"

            elif message_lower.startswith("como") or "o que voc√™ faz" in message_lower:
                return True, f"Sou o Pange.IA - o √∫nico bot que te manda fazer MENOS.\n\nPangeia:\n1Ô∏è‚É£ CUIDAR (voc√™ t√° bem?)\n2Ô∏è‚É£ ORGANIZAR (cortar o desnecess√°rio)\n3Ô∏è‚É£ CRIAR (s√≥ depois de 1 e 2)\n\nN√£o te motivo. Te questiono se vale a pena.\n\nVamos conversar sobre suas tasks ou sobre o que t√° te sobrecarregando?"

            elif message_lower.startswith("tarefas") or message_lower.startswith("task"):
                return True, f"Tasks. Ok.\n\nVou te mostrar suas tasks. Mas depois vamos ter uma conversa honesta sobre quais delas s√£o REALMENTE importantes.\n\nPangeia = cortar o lixo, focar no essencial.\n\nPreparado?"

            elif message_lower.startswith("help") or message_lower.startswith("ajuda"):
                return True, f"Ajuda com o qu√™?\n\nCom tasks? Posso mostrar, criar, marcar como feita.\nCom sobrecarga? Vamos cortar coisas juntos.\nCom procrastina√ß√£o? Vou te perguntar se vale a pena mesmo fazer.\n\nPangeia n√£o √© sobre fazer mais. √â sobre fazer o que IMPORTA.\n\nO que t√° pesando?"

            # Resposta padr√£o disruptiva
            else:
                responses = [
                    f"Entendi.\n\nMas deixa eu te perguntar: isso que voc√™ falou √© importante MESMO ou √© mais uma distra√ß√£o?\n\nPangeia te faz parar e pensar antes de agir.",
                    f"Ok.\n\nAgora a pergunta real: adicionar isso na sua vida te aproxima do que voc√™ QUER ou s√≥ te deixa mais ocupado?\n\nFazer MENOS > fazer MUITO.",
                    f"Saquei.\n\nVoc√™ quer falar sobre isso ou quer que eu te ajude a SIMPLIFICAR tua semana?\n\nPangeia = eliminar o desnecess√°rio. Bora?",
                ]
                return True, responses[hash(message) % len(responses)]

        except Exception as e:
            logger.error(f"Erro ao gerar fallback response: {e}")
            return True, f"Pange.IA aqui. üåç\n\nTe ajudo a fazer MENOS, n√£o mais.\n\nO que t√° pesando?"

    def _track_cost(self, user_id: str, tokens_used: int) -> None:
        """Registra custo de tokens para controle."""
        # Pre√ßo do GPT-4o-mini: $0.00015 por 1K tokens (input/output)
        cost = (tokens_used / 1000) * 0.00015

        if user_id not in self.cost_tracking:
            self.cost_tracking[user_id] = 0.0

        self.cost_tracking[user_id] += cost

        if self.cost_tracking[user_id] > 0.50:  # Log se ultrapassar $0.50
            logger.warning(f"Custo para {user_id}: ${self.cost_tracking[user_id]:.4f}")

    def get_nudge_if_appropriate(
        self,
        person_name: str,
        phone: str,
        context: Optional[Dict] = None
    ) -> Optional[str]:
        """
        Retorna nudge apropriado se for o momento certo.

        Args:
            person_name: Nome da pessoa
            phone: Telefone da pessoa
            context: Contexto adicional (opcional)

        Returns:
            Mensagem de nudge ou None
        """
        try:
            # Obter perfil e m√©tricas
            profile = self.analytics.get_or_create_profile(person_name, phone)

            # Buscar tarefas e calcular m√©tricas psicol√≥gicas
            tasks = self.tasks_manager.get_person_tasks(person_name)
            progress = self.tasks_manager.calculate_progress(person_name)

            # Criar m√©tricas psicol√≥gicas b√°sicas
            from src.psychology.engine import PsychologicalMetrics

            metrics = PsychologicalMetrics()
            metrics.tasks_completed_today = progress.get("concluidas", 0)
            metrics.tasks_pending = progress.get("pendentes", 0)
            metrics.completion_rate = progress.get("percentual", 0) / 100.0

            # Determinar estado emocional aproximado
            total_pendentes = len(tasks.get("a_fazer", [])) + len(tasks.get("em_andamento", []))

            if total_pendentes > 10:
                metrics.emotional_state = EmotionalState.OVERWHELMED
            elif progress.get("percentual", 0) >= 80:
                metrics.emotional_state = EmotionalState.MOTIVATED
            else:
                metrics.emotional_state = EmotionalState.BALANCED

            # Selecionar melhor nudge
            nudge = self.nudge_engine.select_best_nudge(
                person_name,
                profile,
                metrics,
                context or {}
            )

            if not nudge:
                return None

            # Verificar se deve enviar
            should_send = self.personalization_engine.should_send_nudge(
                person_name,
                profile,
                metrics,
                nudge.nudge_type
            )

            if not should_send:
                logger.debug(f"Nudge suprimido para {person_name} - contexto inadequado")
                return None

            # Personalizar nudge
            personalized_nudge = self.personalization_engine.personalize_nudge(
                nudge,
                person_name,
                profile
            )

            logger.info(f"Nudge selecionado para {person_name}: {personalized_nudge.nudge_type.value}")

            return personalized_nudge.message

        except Exception as e:
            logger.error(f"Erro ao buscar nudge: {e}", exc_info=True)
            return None

    def clear_old_memories(self, max_age_hours: int = 24) -> None:
        """Remove hist√≥ricos antigos para liberar mem√≥ria."""
        try:
            removed = self.memory_manager.cleanup_old_conversations(max_age_hours)
            if removed > 0:
                logger.info(f"{removed} conversas antigas removidas")
        except Exception as e:
            logger.error(f"Erro ao limpar hist√≥ricos antigos: {e}")


# Singleton
_agent_instance = None


def get_conversational_agent() -> ConversationalAgent:
    """Obt√©m inst√¢ncia singleton do agente."""
    global _agent_instance
    if _agent_instance is None:
        # Verificar se API key est√° configurada
        if not OPENAI_API_KEY or OPENAI_API_KEY == "":
            logger.warning("‚ö†Ô∏è  OPENAI_API_KEY n√£o configurada - usando fallback")
        _agent_instance = ConversationalAgent()
    return _agent_instance

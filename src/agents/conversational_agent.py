"""
Agente Conversacional com GPT-4o-mini - Conversational Agent.

Sistema de agente que usa LLM (GPT-4o-mini) para gerar respostas naturais
mantendo contexto psicol√≥gico e intelig√™ncia emocional.
"""

import logging
import json
from typing import Optional, Tuple, Dict, List
from datetime import datetime, timedelta
from collections import deque

from config.openai_config import (
    client, GPT_MODEL, OPENAI_API_KEY, MAX_CONTEXT_TOKENS, MAX_RESPONSE_TOKENS,
    TEMPERATURE, TOP_P, REQUEST_TIMEOUT, MAX_CONVERSATION_HISTORY,
    SYSTEM_PROMPT_TEMPLATE, FALLBACK_RESPONSES, LOG_CONVERSATIONS,
    APIError, APIConnectionError, RateLimitError, AuthenticationError
)
from src.psychology.engine import PsychologicalEngine
from src.people.analytics import PeopleAnalytics

logger = logging.getLogger(__name__)


class ConversationalAgent:
    """
    Agente conversacional com mem√≥ria e contexto psicol√≥gico.

    Usa GPT-4o-mini para gerar respostas naturais enquanto mant√©m:
    - Hist√≥rico de conversa
    - Contexto psicol√≥gico
    - An√°lise emocional
    - Limita√ß√µes de tokens
    """

    def __init__(self):
        """Inicializa o agente conversacional."""
        self.psych_engine = PsychologicalEngine()
        self.analytics = PeopleAnalytics()

        # Memory por usu√°rio: {user_id: deque de mensagens}
        self.conversation_memory: Dict[str, deque] = {}

        # Tracking de custos
        self.cost_tracking: Dict[str, float] = {}

        # √öltimas respostas (para evitar repeti√ß√£o)
        self.response_cache: Dict[str, Tuple[str, datetime]] = {}

        logger.info("ConversationalAgent inicializado com GPT-4o-mini")

    def get_conversation_history(self, user_id: str) -> List[Dict]:
        """Obt√©m hist√≥rico da conversa do usu√°rio."""
        if user_id not in self.conversation_memory:
            self.conversation_memory[user_id] = deque(maxlen=MAX_CONVERSATION_HISTORY)

        return list(self.conversation_memory[user_id])

    def add_to_memory(self, user_id: str, role: str, content: str) -> None:
        """Adiciona mensagem ao hist√≥rico."""
        if user_id not in self.conversation_memory:
            self.conversation_memory[user_id] = deque(maxlen=MAX_CONVERSATION_HISTORY)

        self.conversation_memory[user_id].append({
            "role": role,
            "content": content,
            "timestamp": datetime.now().isoformat()
        })

    def _safe_analyze_user(self, person_name: str) -> Dict:
        """
        Wrapper seguro para an√°lise psicol√≥gica do usu√°rio.
        Retorna dicion√°rio com dados mesmo se an√°lise falhar.
        """
        try:
            # Tenta obter perfil
            profile = self.analytics.get_or_create_profile(person_name)

            # Dados padr√£o se tudo falhar
            return {
                "emotional_state": "Tranquilo",
                "energy_level": "boa",
                "active_tasks": "algumas tarefas",
                "progress": "em progresso"
            }
        except Exception as e:
            logger.debug(f"N√£o foi poss√≠vel analisar usu√°rio {person_name}: {e}")
            # Retorna sempre dados padr√£o
            return {
                "emotional_state": "Tranquilo",
                "energy_level": "boa",
                "active_tasks": "algumas tarefas",
                "progress": "em progresso"
            }

    def build_system_prompt(self, person_name: str) -> str:
        """Constr√≥i system prompt personalizado com contexto."""
        try:
            # An√°lise psicol√≥gica segura (sempre retorna dados)
            metrics = self._safe_analyze_user(person_name)

            # Preenche template com dados seguros
            prompt = SYSTEM_PROMPT_TEMPLATE.format(
                name=person_name or "Amigo",
                emotional_state=metrics.get("emotional_state", "Tranquilo"),
                energy_level=metrics.get("energy_level", "boa"),
                active_tasks=metrics.get("active_tasks", "algumas tarefas"),
                progress=metrics.get("progress", "em progresso")
            )

            return prompt

        except Exception as e:
            logger.error(f"Erro ao construir system prompt: {e}")
            # Fallback seguro
            return SYSTEM_PROMPT_TEMPLATE.format(
                name=person_name or "Amigo",
                emotional_state="Tranquilo",
                energy_level="boa",
                active_tasks="algumas tarefas",
                progress="em progresso"
            )

    def generate_response(self, user_id: str, message: str, person_name: str) -> Tuple[bool, str]:
        """
        Gera resposta usando GPT-4o-mini com contexto conversacional.

        Args:
            user_id: ID √∫nico do usu√°rio
            message: Mensagem recebida
            person_name: Nome da pessoa

        Returns:
            Tuple (sucesso, resposta)
        """
        try:
            # Verificar se cliente est√° dispon√≠vel
            if not client:
                logger.warning("‚ö†Ô∏è Cliente OpenAI n√£o inicializado - usando fallback")
                return self._generate_fallback_response(message, person_name)

            # Adiciona mensagem do usu√°rio ao hist√≥rico
            self.add_to_memory(user_id, "user", message)

            # Obt√©m hist√≥rico
            history = self.get_conversation_history(user_id)

            # Constr√≥i system prompt personalizado
            system_prompt = self.build_system_prompt(person_name)

            # Verifica cache para evitar resposta duplicada
            cache_key = f"{user_id}:{message}"
            if cache_key in self.response_cache:
                cached_response, timestamp = self.response_cache[cache_key]
                if (datetime.now() - timestamp).seconds < 60:  # Cache de 1 minuto
                    logger.info(f"Resposta do cache para {person_name}")
                    return True, cached_response

            # Chamada ao GPT-4o-mini com novo cliente (v1.0+)
            logger.info(f"Gerando resposta com GPT-4o-mini para {person_name}")

            response = client.chat.completions.create(
                model=GPT_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    *[{"role": msg["role"], "content": msg["content"]} for msg in history],
                ],
                temperature=TEMPERATURE,
                top_p=TOP_P,
                max_tokens=MAX_RESPONSE_TOKENS,
                timeout=REQUEST_TIMEOUT,
            )

            # Extrai resposta
            bot_response = response.choices[0].message.content.strip()

            # Adiciona resposta do bot ao hist√≥rico
            self.add_to_memory(user_id, "assistant", bot_response)

            # Calcula e registra tokens usados
            if hasattr(response, 'usage'):
                tokens_used = response.usage.total_tokens
                logger.info(f"Tokens utilizados: {tokens_used}")
                self._track_cost(user_id, tokens_used)

            # Cache a resposta
            self.response_cache[cache_key] = (bot_response, datetime.now())

            # Log da conversa se habilitado
            if LOG_CONVERSATIONS:
                logger.info(f"[{person_name}] User: {message} | Bot: {bot_response[:100]}...")

            return True, bot_response

        except AuthenticationError:
            logger.error(f"Erro de autentica√ß√£o OpenAI - verifique OPENAI_API_KEY")
            return self._generate_fallback_response(message, person_name)

        except RateLimitError:
            logger.warning(f"Rate limit atingido para {person_name}")
            return False, FALLBACK_RESPONSES["overload"]

        except APIConnectionError as e:
            logger.warning(f"Erro de conex√£o com OpenAI para {person_name}: {e}")
            return False, FALLBACK_RESPONSES["timeout"]

        except APIError as e:
            logger.error(f"Erro na API OpenAI: {e}")
            return self._generate_fallback_response(message, person_name)

        except Exception as e:
            logger.error(f"Erro ao gerar resposta: {e}")
            return self._generate_fallback_response(message, person_name)

    def _generate_fallback_response(self, message: str, person_name: str) -> Tuple[bool, str]:
        """
        Gera resposta de fallback quando GPT-4o-mini n√£o est√° dispon√≠vel.
        Mant√©m conversa natural mesmo sem IA.
        """
        try:
            message_lower = message.lower().strip()

            # Respostas conversacionais simples
            if any(word in message_lower for word in ["oi", "opa", "ol√°", "e a√≠", "eae"]):
                responses = [
                    f"Oi, {person_name}! üëã Tudo bem com voc√™?",
                    f"E a√≠, {person_name}? Quero ouvir como voc√™ est√°! üòä",
                    f"Opa, {person_name}! Bora conversar? üíô",
                ]
                return True, responses[hash(message) % len(responses)]

            elif any(word in message_lower for word in ["tchau", "at√©", "falou", "bye"]):
                responses = [
                    f"Falou, {person_name}! Fico por aqui, mas volta quando quiser! üëã",
                    f"At√© mais, {person_name}! Cuida de voc√™! üíô",
                ]
                return True, responses[hash(message) % len(responses)]

            elif any(word in message_lower for word in ["obrigado", "obg", "vlw", "valeu"]):
                return True, f"De nada, {person_name}! Fico feliz em ajudar! üòä"

            elif message_lower.startswith("como"):
                return True, f"{person_name}, fico feliz que me perguntou! Mas me conta primeiro como VOC√ä est√°? ü§î"

            elif message_lower.startswith("tarefas") or message_lower.startswith("task"):
                return True, f"Vamos conversar sobre suas tarefas? Qual √© a que mais est√° te preocupando agora? üí≠"

            elif message_lower.startswith("help") or message_lower.startswith("ajuda"):
                return True, f"Claro, {person_name}! Estou aqui para ajudar com suas tarefas e tamb√©m pra ouvir como voc√™ est√°. Qual √©? üëÇ"

            # Resposta padr√£o conversacional
            else:
                responses = [
                    f"Entendi! Me conta mais sobre isso, {person_name}... üëÇ",
                    f"Que interessante, {person_name}! Conta pra mim com mais detalhes ü§î",
                    f"Ah, {person_name}... isso soa importante. O que voc√™ acha? üí≠",
                ]
                return True, responses[hash(message) % len(responses)]

        except Exception as e:
            logger.error(f"Erro ao gerar fallback response: {e}")
            return True, f"Oi, {person_name}! T√¥ aqui pra voc√™, pode contar comigo üíô"

    def _track_cost(self, user_id: str, tokens_used: int) -> None:
        """Registra custo de tokens para controle."""
        # Pre√ßo do GPT-4o-mini: $0.00015 por 1K tokens (input/output)
        cost = (tokens_used / 1000) * 0.00015

        if user_id not in self.cost_tracking:
            self.cost_tracking[user_id] = 0.0

        self.cost_tracking[user_id] += cost

        if self.cost_tracking[user_id] > 0.50:  # Log se ultrapassar $0.50
            logger.warning(f"Custo para {user_id}: ${self.cost_tracking[user_id]:.4f}")

    def clear_old_memories(self, max_age_hours: int = 24) -> None:
        """Remove hist√≥ricos antigos para liberar mem√≥ria."""
        try:
            cutoff_time = datetime.now() - timedelta(hours=max_age_hours)

            for user_id, memory in list(self.conversation_memory.items()):
                # Filtra mensagens antigas
                filtered = deque(
                    [msg for msg in memory
                     if datetime.fromisoformat(msg["timestamp"]) > cutoff_time],
                    maxlen=MAX_CONVERSATION_HISTORY
                )

                if len(filtered) == 0:
                    del self.conversation_memory[user_id]
                    logger.info(f"Hist√≥rico de {user_id} removido")
                else:
                    self.conversation_memory[user_id] = filtered

        except Exception as e:
            logger.error(f"Erro ao limpar hist√≥ricos antigos: {e}")


# Singleton
_agent_instance = None


def get_conversational_agent() -> ConversationalAgent:
    """Obt√©m inst√¢ncia singleton do agente."""
    global _agent_instance
    if _agent_instance is None:
        # Verificar se API key est√° configurada
        if not OPENAI_API_KEY or OPENAI_API_KEY == "":
            logger.warning("‚ö†Ô∏è  OPENAI_API_KEY n√£o configurada - usando fallback")
        _agent_instance = ConversationalAgent()
    return _agent_instance
